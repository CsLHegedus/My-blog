So I saw a good idea on youtube, a challenge to learn machine learning for 66 consecutive days, just for 5 minutes.  

Before I get into that. I mentioned a course before. 
Well, the course had some advices about what kind of 
people are good as data analyst, statistian and machine 
learning engineer. I still think that the third option 
fits me the best. But now I'm without courses. I could 
go for the overlapping parts, but I focus on making my 
own "course".  

Anyway, first day, I'll write hopefully everyday for a while from now on.  



## Day 2.2
I spent an hour setting up OBS (video recorder) again. 
My previous learning/online diary videos were quite good feedback, so in a bit different format I try that again.
I'll make them with less talking, more like a stream. My new pc and mic is a huge help with that.
I really need to figure out how to format these posts, before I write too much.

Other than that I'll start the data analyst course again at 27th of Febr.
My reasons are these overlapping fields with data engineering:
* My data exploration skills are still lacking, what I can do I do too slowly.
* My research skillset is also quite weak.
* SQL in practice
* Data visualization tools.
* Professional communication.

I'll spend 5 minutes with the data analyst course and 5 minutes of machine learning engineering every day from that day!

In the meanwhile I plan to get to at least to the tensorflow functional APIs.

## Day 2.1
I skipped two days, so it's day one again. (Try: 2nd, day 1)
I started to practice a bit with tensorflow's sequential api. 
That's it for now.

## Day 3
I didn't have much time today, I highlighted the more useful parts in the first four chap. of the ml. eng. book.
I'll create some cheat sheets from the summaries of the book on the weekend.

## Day 2
Not much today, I read the machine learning engineering book, up to the feature crafting chapter.
I also went through some of my old google colab projects. I aactually understood most of it!

Other than that I'll probably cut the chase and get a deep dive. 
I'll go through the only scientific paper I worked before instead of creating a full fledged course. 
The rest of the plans are the same. 

Get to the point where I can re-create an architecture from a paper.
Deploy the model
SQL/ Data lake
Some real data, I think

## Day 1 - Humble beginnings, or re-re-re-re-opening 
I picked up again Andriy Burkov'S "Machine Learning Engineering" book. 
I had some experience with deeplearning and his book is kind of a 
toolbox.  It has a simple, still very useful framework, it's full of 
good practice advices and it covers the full life cycle of a model to
give some perspective. 
It also has a nice glossary, to refresh vocab of data science.  

So to quote some ideas from the book , the part of creating a product: 
in short 
*decision making, product management 
*data engineering 
*prototype phase ml engineering 
*statistics 
*production phase ml engineering
*reliability engineering  

Other than that I was brainstorming about the content of my course. I'll use mrdbourke's course as basis and extend it here and there 
This is just a draft, sort of  
Stage 1
dnn models - simple linear regression, parts of the model in python, tensorflow, pytorch, first iteration of the workflow,  
different activation functions, non-linearity, first helper fuctions? python, tensorflow, pytorch, use of documentation 
cnn models, parts, first famous architectures (alexnet, vgg16) lstm, gru, etc. use cases some paper here?   
transfer learning - feature extraction, transfer learning

some full fledged papers? when use sequential api and functional api?

Stage 2 deploy a model, barebone website
Stage 3 polished, presentable website?
Stage 4 SQL/ data lake + previous ones
Stage 5 real life data -> database/datalake -> model -> one retraining 
